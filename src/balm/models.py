'''
Created on Aug 7, 2023

@author: immanueltrummer
'''
import json
import pathlib

from io import StringIO
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chat_models.base import BaseChatModel
from langchain.schema import HumanMessage


class Models():
    """ Wrapper around language models from multiple providers. """
    
    def __init__(self, root_path, credentials):
        """ Initializes models by reading configuration file. 
        
        Args:
            root_path: root directory of BALM.
            credentials: dictionary mapping API names to access keys.
        """
        self.credentials = credentials
        config_path = pathlib.Path(root_path, 'configuration', 'models.json')
        with open(config_path) as config_file:
            self.model_config = json.load(config_file)
            
        self.label2model = {}
    
    def available_models(self):
        """ Returns list of available model names, given credentials.
        
        Returns:
            list of names of available models.
        """
        available = []
        for model_name, model_info in self.model_config.items():
            provider = model_info['provider']
            if provider in self.credentials and self.credentials[provider]:
                available.append(model_name)
        return available
                
    def apply_model(self, model_label, file, task):
        """ Applies model to data for given task and returns answer.
        
        Args:
            model_label: display label of model to apply.
            file: solve task for this file.
            task: task description in natural language.
        
        Returns:
            answer string generated by model.
        """
        llm = self._get_model(model_label)
        text = StringIO(file.getvalue().decode('utf-8')).read()
        prompt = text + '\n' + task + '\n'
        if isinstance(llm, BaseChatModel):
            in_msg = HumanMessage(content=prompt)
            out_msg = llm([in_msg])
            return out_msg.content
        else:
            return llm(prompt)
    
    def _get_model(self, model_label):
        """ Retrieve or generate model with given name. 
        
        Args:
            model_label: display label of model.
        
        Returns:
            a LangChain language model object.
        """
        model_info = self.model_config[model_label]
        model_name = model_info['name']
        provider = model_info['provider']
        model_type = model_info['type']
        api_key = self.credentials[provider]
        
        if provider == 'OpenAI':
            if model_type == 'chat':
                return ChatOpenAI(
                    model_name=model_name, openai_api_key=api_key)
            else:
                return OpenAI(model_name=model_name, openai_api_key=api_key)
        else:
            raise ValueError(f'Unsupported model provider: {provider}')